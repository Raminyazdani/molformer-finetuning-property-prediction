{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration of Data Selection and Fine-tuning Strategies\n",
        "\n",
        "In this task, you can explore alternative methods for data selection and investigate several fine-tuning techniques to adapt a pre-trained model for a specific task. The goal is to improve the model performance on our target dataset. You can use the [`task3.py`](../scripts/Task3.py) file for your implementation.\n",
        "\n",
        "1. Data Selection Strategies\n",
        "The first step in fine-tuning a model is to carefully select the training data. While the previous tasks focused on influence-based data selection, here you will experiment with other selection strategies. Pick one data selection method by yourself. Log your findings about the selected data subsets:\n",
        "    - How much data is used in each strategy?\n",
        "    - Compare the performance of models trained with each selection method.\n",
        "\n",
        "2. Fine-tuning Strategies\n",
        "In this section, you will implement and compare some parameter-efficient fine-tuning approaches:\n",
        "\n",
        "    - [bitfit](https://arxiv.org/abs/2106.10199)\n",
        "    - [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation)\n",
        "    - iA3 (see Section 3.3. of this [paper](https://arxiv.org/abs/2205.05638)) (Implicit Adapter)"
      ],
      "metadata": {
        "id": "AcJqSrvZwEwy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyFMTF1HwBU_",
        "outputId": "f5bae6e6-8f73-45d0-f936-d55ef6147839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/content/drive/')# Note: Commented out for local execution. Uncomment if using Google Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y64hpBgwgcr",
        "outputId": "541e3fb5-a78d-4e29-8c2e-27328841625d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfkg-ciGwy1t",
        "outputId": "8f39c2f4-8c43-4e5e-8414-308b67d3f267"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.1.0)\n",
            "Downloading rdkit-2024.9.5-cp311-cp311-manylinux_2_28_x86_64.whl (34.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sklearn\n",
        "!pip install torch\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-coqtaXcFgv8",
        "outputId": "1db14074-4460-4a8d-b436-4aa72039c9be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.7)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.6)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.22.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "from rdkit import RDLogger\n",
        "\n",
        "# Suppress RDKit warnings\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Import dependencies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdMolDescriptors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Define the device (GPU if available, otherwise CPU)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "MODEL_NAME = \"ibm/MoLFormer-XL-both-10pct\"\n",
        "\n",
        "########################################################\n",
        "# Data Selection Strategies\n",
        "########################################################\n",
        "\n",
        "def random_sampling(smiles_list, targets, sample_size):\n",
        "    \"\"\"Randomly select a subset of the data.\"\"\"\n",
        "    indices = np.random.choice(len(smiles_list), size=sample_size, replace=False)\n",
        "    return [smiles_list[i] for i in indices], [targets[i] for i in indices]\n",
        "\n",
        "def diversity_sampling(smiles_list, targets, sample_size):\n",
        "    \"\"\"Select a diverse subset of the data using molecular fingerprints.\"\"\"\n",
        "    # Generate Morgan fingerprints\n",
        "    fingerprints = []\n",
        "    for smiles in smiles_list:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is not None:\n",
        "            fp = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)\n",
        "            fingerprints.append(np.array(fp))\n",
        "        else:\n",
        "            fingerprints.append(np.zeros(1024, dtype=int))  # Handle invalid SMILES\n",
        "\n",
        "    fingerprints = np.array(fingerprints)\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    kmeans = KMeans(n_clusters=sample_size, random_state=42)\n",
        "    kmeans.fit(fingerprints)\n",
        "\n",
        "    # Select one sample from each cluster\n",
        "    selected_indices = []\n",
        "    for cluster in range(sample_size):\n",
        "        cluster_indices = np.where(kmeans.labels_ == cluster)[0]\n",
        "        selected_indices.append(np.random.choice(cluster_indices))\n",
        "\n",
        "    return [smiles_list[i] for i in selected_indices], [targets[i] for i in selected_indices]\n",
        "\n",
        "########################################################\n",
        "# Custom Dataset Class\n",
        "########################################################\n",
        "\n",
        "class SMILESDataset(Dataset):\n",
        "    def __init__(self, smiles_list, targets, tokenizer, max_length=128):\n",
        "        self.smiles_list = smiles_list\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.smiles_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        smiles = self.smiles_list[idx]\n",
        "        target = self.targets[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            smiles,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # Remove batch dimension\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"target\": torch.tensor(target, dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "########################################################\n",
        "# Fine-Tuning Strategies\n",
        "########################################################\n",
        "\n",
        "# Add a regression head to the model\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, base_model, hidden_size=768):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.regressor = nn.Linear(hidden_size, 1)  # Output shape: (batch_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.base_model(input_ids, attention_mask)\n",
        "        pooled_output = outputs.pooler_output  # Shape: (batch_size, hidden_size)\n",
        "        return self.regressor(pooled_output).squeeze(-1)  # Ensure shape: (batch_size,)\n",
        "\n",
        "def bitfit_finetune(model, train_loader, num_epochs=100, lr=5e-4):\n",
        "    \"\"\"Fine-tune only the bias terms of the model.\"\"\"\n",
        "    bias_params = [p for n, p in model.named_parameters() if 'bias' in n]\n",
        "    optimizer = AdamW(bias_params, lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            targets = batch['target'].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            loss = criterion(predictions, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "def lora_finetune(model, train_loader, num_epochs=100, lr=1e-5, rank=8):\n",
        "    \"\"\"Fine-tune the model using LoRA (Low-Rank Adaptation).\"\"\"\n",
        "\n",
        "    lora_params = {}\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            lora_name = name.replace(\".\", \"_\")  # Fix parameter naming issue\n",
        "\n",
        "            lora_A = nn.Parameter(torch.randn(rank, module.in_features).to(DEVICE) * 0.01)  # Scale to avoid NaNs\n",
        "            lora_B = nn.Parameter(torch.randn(module.out_features, rank).to(DEVICE) * 0.01)\n",
        "\n",
        "            module.weight.requires_grad = False  # Freeze original weights\n",
        "\n",
        "            lora_params[f\"{lora_name}_lora_A\"] = lora_A\n",
        "            lora_params[f\"{lora_name}_lora_B\"] = lora_B\n",
        "\n",
        "    for param_name, param in lora_params.items():\n",
        "        model.register_parameter(param_name, param)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            targets = batch['target'].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model.base_model(input_ids, attention_mask)\n",
        "            pooled_output = outputs.pooler_output  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "            for name, module in model.named_modules():\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    lora_name = name.replace(\".\", \"_\")\n",
        "                    lora_A = getattr(model, f\"{lora_name}_lora_A\")\n",
        "                    lora_B = getattr(model, f\"{lora_name}_lora_B\")\n",
        "\n",
        "                    lora_output = torch.matmul(pooled_output, lora_A.T)\n",
        "                    lora_output = torch.matmul(lora_output, lora_B.T)\n",
        "\n",
        "                    pooled_output = pooled_output + lora_output  # Apply LoRA\n",
        "\n",
        "            # Normalize to prevent NaNs\n",
        "            pooled_output = torch.nn.functional.layer_norm(pooled_output, (pooled_output.shape[-1],))\n",
        "\n",
        "            predictions = model.regressor(pooled_output).squeeze(-1)\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Prevent NaNs\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "\n",
        "def ia3_finetune(model, train_loader, num_epochs=100, lr=5e-4):\n",
        "    \"\"\"Fine-tune the model using iA3 (Implicit Adapter).\"\"\"\n",
        "\n",
        "    ia3_params = {}\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            ia3_name = name.replace(\".\", \"_\")  # Fix parameter naming issue\n",
        "            ia3_scale = nn.Parameter(torch.ones(module.out_features).to(DEVICE))\n",
        "\n",
        "            module.weight.requires_grad = False  # Freeze original weights\n",
        "            ia3_params[f\"{ia3_name}_ia3_scale\"] = ia3_scale\n",
        "\n",
        "    for param_name, param in ia3_params.items():\n",
        "        model.register_parameter(param_name, param)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            targets = batch['target'].to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model.base_model(input_ids, attention_mask)\n",
        "            pooled_output = outputs.pooler_output  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "            for name, module in model.named_modules():\n",
        "                if isinstance(module, nn.Linear):\n",
        "                    ia3_name = name.replace(\".\", \"_\")\n",
        "                    ia3_scale = getattr(model, f\"{ia3_name}_ia3_scale\")\n",
        "\n",
        "                    pooled_output = pooled_output * ia3_scale  # Apply iA3 scaling\n",
        "\n",
        "            predictions = model.regressor(pooled_output).squeeze(-1)\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "########################################################\n",
        "# Evaluation Function\n",
        "########################################################\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(DEVICE)\n",
        "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
        "            targets = batch['target'].to(DEVICE)\n",
        "\n",
        "            predictions = model(input_ids, attention_mask)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    mse = mean_squared_error(all_targets, all_predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(all_targets, all_predictions)\n",
        "    r2 = r2_score(all_targets, all_predictions)\n",
        "\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "    return mse, rmse, mae, r2\n",
        "\n",
        "########################################################\n",
        "# Main Execution\n",
        "########################################################\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the MoleculeNet Lipophilicity dataset from Hugging Face\n",
        "    dataset = load_dataset(\"scikit-fingerprints/MoleculeNet_Lipophilicity\")\n",
        "\n",
        "    # Extract SMILES and targets\n",
        "    smiles_list = dataset[\"train\"][\"SMILES\"]\n",
        "    targets = dataset[\"train\"][\"label\"]\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    train_smiles, test_smiles, train_targets, test_targets = train_test_split(\n",
        "        smiles_list, targets, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Apply data selection strategies\n",
        "    random_smiles, random_targets = random_sampling(train_smiles, train_targets, sample_size=1000)\n",
        "    diverse_smiles, diverse_targets = diversity_sampling(train_smiles, train_targets, sample_size=1000)\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "    # Create datasets\n",
        "    random_dataset = SMILESDataset(random_smiles, random_targets, tokenizer)\n",
        "    diverse_dataset = SMILESDataset(diverse_smiles, diverse_targets, tokenizer)\n",
        "    test_dataset = SMILESDataset(test_smiles, test_targets, tokenizer)\n",
        "\n",
        "    # Create data loaders\n",
        "    random_loader = DataLoader(random_dataset, batch_size=64, shuffle=True)\n",
        "    diverse_loader = DataLoader(diverse_dataset, batch_size=64, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Load the pre-trained model and add the regression head\n",
        "    base_model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(DEVICE)\n",
        "    model = RegressionModel(base_model).to(DEVICE)\n",
        "\n",
        "    # Fine-tune and evaluate each method separately\n",
        "    print(\"Evaluating BitFit...\")\n",
        "    bitfit_model = RegressionModel(AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(DEVICE)).to(DEVICE)\n",
        "    bitfit_finetune(bitfit_model, random_loader)\n",
        "    evaluate_model(bitfit_model, test_loader)\n",
        "\n",
        "    print(\"Evaluating LoRA...\")\n",
        "    lora_model = RegressionModel(AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(DEVICE)).to(DEVICE)\n",
        "    lora_finetune(lora_model, diverse_loader)\n",
        "    evaluate_model(lora_model, test_loader)\n",
        "\n",
        "    print(\"Evaluating iA3...\")\n",
        "    ia3_model = RegressionModel(AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True).to(DEVICE)).to(DEVICE)\n",
        "    ia3_finetune(ia3_model, random_loader)\n",
        "    evaluate_model(ia3_model, test_loader)"
      ],
      "metadata": {
        "id": "Lxd4WPDfwZa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4413c22e-25ea-4258-ec29-ac74016a15c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The repository for ibm/MoLFormer-XL-both-10pct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ibm/MoLFormer-XL-both-10pct.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n",
            "Evaluating BitFit...\n",
            "Epoch [1/100], Loss: 5.2422\n",
            "Epoch [2/100], Loss: 1.6940\n",
            "Epoch [3/100], Loss: 1.5218\n",
            "Epoch [4/100], Loss: 1.4067\n",
            "Epoch [5/100], Loss: 1.4152\n",
            "Epoch [6/100], Loss: 1.3792\n",
            "Epoch [7/100], Loss: 1.3455\n",
            "Epoch [8/100], Loss: 1.3271\n",
            "Epoch [9/100], Loss: 1.3039\n",
            "Epoch [10/100], Loss: 1.3003\n",
            "Epoch [11/100], Loss: 1.2863\n",
            "Epoch [12/100], Loss: 1.2750\n",
            "Epoch [13/100], Loss: 1.2585\n",
            "Epoch [14/100], Loss: 1.2607\n",
            "Epoch [15/100], Loss: 1.2197\n",
            "Epoch [16/100], Loss: 1.2172\n",
            "Epoch [17/100], Loss: 1.2008\n",
            "Epoch [18/100], Loss: 1.1858\n",
            "Epoch [19/100], Loss: 1.1819\n",
            "Epoch [20/100], Loss: 1.1740\n",
            "Epoch [21/100], Loss: 1.1983\n",
            "Epoch [22/100], Loss: 1.1608\n",
            "Epoch [23/100], Loss: 1.1343\n",
            "Epoch [24/100], Loss: 1.1165\n",
            "Epoch [25/100], Loss: 1.0998\n",
            "Epoch [26/100], Loss: 1.0868\n",
            "Epoch [27/100], Loss: 1.0953\n",
            "Epoch [28/100], Loss: 1.0990\n",
            "Epoch [29/100], Loss: 1.1040\n",
            "Epoch [30/100], Loss: 1.0853\n",
            "Epoch [31/100], Loss: 1.0348\n",
            "Epoch [32/100], Loss: 1.0475\n",
            "Epoch [33/100], Loss: 1.0222\n",
            "Epoch [34/100], Loss: 0.9938\n",
            "Epoch [35/100], Loss: 0.9870\n",
            "Epoch [36/100], Loss: 0.9973\n",
            "Epoch [37/100], Loss: 0.9898\n",
            "Epoch [38/100], Loss: 0.9770\n",
            "Epoch [39/100], Loss: 0.9870\n",
            "Epoch [40/100], Loss: 0.9452\n",
            "Epoch [41/100], Loss: 0.9624\n",
            "Epoch [42/100], Loss: 0.9279\n",
            "Epoch [43/100], Loss: 0.9530\n",
            "Epoch [44/100], Loss: 0.9574\n",
            "Epoch [45/100], Loss: 0.9229\n",
            "Epoch [46/100], Loss: 0.9456\n",
            "Epoch [47/100], Loss: 0.9339\n",
            "Epoch [48/100], Loss: 0.8934\n",
            "Epoch [49/100], Loss: 0.9073\n",
            "Epoch [50/100], Loss: 0.9483\n",
            "Epoch [51/100], Loss: 0.9075\n",
            "Epoch [52/100], Loss: 0.9086\n",
            "Epoch [53/100], Loss: 0.8977\n",
            "Epoch [54/100], Loss: 0.9091\n",
            "Epoch [55/100], Loss: 0.8808\n",
            "Epoch [56/100], Loss: 0.9027\n",
            "Epoch [57/100], Loss: 0.8609\n",
            "Epoch [58/100], Loss: 0.9266\n",
            "Epoch [59/100], Loss: 0.8687\n",
            "Epoch [60/100], Loss: 0.8350\n",
            "Epoch [61/100], Loss: 0.8655\n",
            "Epoch [62/100], Loss: 0.8746\n",
            "Epoch [63/100], Loss: 0.8934\n",
            "Epoch [64/100], Loss: 0.8601\n",
            "Epoch [65/100], Loss: 0.8839\n",
            "Epoch [66/100], Loss: 0.8440\n",
            "Epoch [67/100], Loss: 0.8535\n",
            "Epoch [68/100], Loss: 0.8376\n",
            "Epoch [69/100], Loss: 0.8363\n",
            "Epoch [70/100], Loss: 0.8399\n",
            "Epoch [71/100], Loss: 0.8288\n",
            "Epoch [72/100], Loss: 0.8358\n",
            "Epoch [73/100], Loss: 0.7955\n",
            "Epoch [74/100], Loss: 0.7905\n",
            "Epoch [75/100], Loss: 0.8052\n",
            "Epoch [76/100], Loss: 0.8002\n",
            "Epoch [77/100], Loss: 0.8310\n",
            "Epoch [78/100], Loss: 0.8180\n",
            "Epoch [79/100], Loss: 0.7997\n",
            "Epoch [80/100], Loss: 0.7776\n",
            "Epoch [81/100], Loss: 0.8447\n",
            "Epoch [82/100], Loss: 0.8166\n",
            "Epoch [83/100], Loss: 0.7696\n",
            "Epoch [84/100], Loss: 0.7683\n",
            "Epoch [85/100], Loss: 0.7534\n",
            "Epoch [86/100], Loss: 0.7596\n",
            "Epoch [87/100], Loss: 0.7498\n",
            "Epoch [88/100], Loss: 0.7630\n",
            "Epoch [89/100], Loss: 0.7989\n",
            "Epoch [90/100], Loss: 0.7370\n",
            "Epoch [91/100], Loss: 0.7656\n",
            "Epoch [92/100], Loss: 0.7433\n",
            "Epoch [93/100], Loss: 0.7332\n",
            "Epoch [94/100], Loss: 0.7315\n",
            "Epoch [95/100], Loss: 0.7285\n",
            "Epoch [96/100], Loss: 0.7098\n",
            "Epoch [97/100], Loss: 0.7294\n",
            "Epoch [98/100], Loss: 0.7430\n",
            "Epoch [99/100], Loss: 0.7301\n",
            "Epoch [100/100], Loss: 0.7789\n",
            "Mean Squared Error (MSE): 0.9609\n",
            "Root Mean Squared Error (RMSE): 0.9803\n",
            "Mean Absolute Error (MAE): 0.7856\n",
            "R² Score: 0.3496\n",
            "Evaluating LoRA...\n",
            "Epoch [1/100], Loss: 6.0842\n",
            "Epoch [2/100], Loss: 4.1329\n",
            "Epoch [3/100], Loss: 2.5643\n",
            "Epoch [4/100], Loss: 1.5291\n",
            "Epoch [5/100], Loss: 1.3866\n",
            "Epoch [6/100], Loss: 1.3398\n",
            "Epoch [7/100], Loss: 1.2731\n",
            "Epoch [8/100], Loss: 1.2400\n",
            "Epoch [9/100], Loss: 1.1895\n",
            "Epoch [10/100], Loss: 1.1627\n",
            "Epoch [11/100], Loss: 1.1521\n",
            "Epoch [12/100], Loss: 1.1056\n",
            "Epoch [13/100], Loss: 1.1020\n",
            "Epoch [14/100], Loss: 1.0849\n",
            "Epoch [15/100], Loss: 1.0565\n",
            "Epoch [16/100], Loss: 1.0662\n",
            "Epoch [17/100], Loss: 1.0474\n",
            "Epoch [18/100], Loss: 1.0318\n",
            "Epoch [19/100], Loss: 0.9995\n",
            "Epoch [20/100], Loss: 0.9829\n",
            "Epoch [21/100], Loss: 0.9850\n",
            "Epoch [22/100], Loss: 0.9564\n",
            "Epoch [23/100], Loss: 0.9558\n",
            "Epoch [24/100], Loss: 0.9574\n",
            "Epoch [25/100], Loss: 0.9393\n",
            "Epoch [26/100], Loss: 0.9330\n",
            "Epoch [27/100], Loss: 0.9212\n",
            "Epoch [28/100], Loss: 0.9235\n",
            "Epoch [29/100], Loss: 0.9270\n",
            "Epoch [30/100], Loss: 0.8974\n",
            "Epoch [31/100], Loss: 0.8850\n",
            "Epoch [32/100], Loss: 0.8743\n",
            "Epoch [33/100], Loss: 0.8816\n",
            "Epoch [34/100], Loss: 0.8759\n",
            "Epoch [35/100], Loss: 0.8895\n",
            "Epoch [36/100], Loss: 0.8476\n",
            "Epoch [37/100], Loss: 0.8336\n",
            "Epoch [38/100], Loss: 0.8524\n",
            "Epoch [39/100], Loss: 0.8640\n",
            "Epoch [40/100], Loss: 0.8429\n",
            "Epoch [41/100], Loss: 0.8335\n",
            "Epoch [42/100], Loss: 0.8476\n",
            "Epoch [43/100], Loss: 0.8242\n",
            "Epoch [44/100], Loss: 0.8383\n",
            "Epoch [45/100], Loss: 0.8314\n",
            "Epoch [46/100], Loss: 0.8037\n",
            "Epoch [47/100], Loss: 0.7980\n",
            "Epoch [48/100], Loss: 0.7916\n",
            "Epoch [49/100], Loss: 0.7847\n",
            "Epoch [50/100], Loss: 0.7929\n",
            "Epoch [51/100], Loss: 0.7903\n",
            "Epoch [52/100], Loss: 0.7893\n",
            "Epoch [53/100], Loss: 0.7861\n",
            "Epoch [54/100], Loss: 0.7860\n",
            "Epoch [55/100], Loss: 0.7710\n",
            "Epoch [56/100], Loss: 0.7902\n",
            "Epoch [57/100], Loss: 0.7529\n",
            "Epoch [58/100], Loss: 0.7536\n",
            "Epoch [59/100], Loss: 0.7839\n",
            "Epoch [60/100], Loss: 0.7463\n",
            "Epoch [61/100], Loss: 0.7663\n",
            "Epoch [62/100], Loss: 0.7482\n",
            "Epoch [63/100], Loss: 0.7478\n",
            "Epoch [64/100], Loss: 0.7533\n",
            "Epoch [65/100], Loss: 0.7518\n",
            "Epoch [66/100], Loss: 0.7503\n",
            "Epoch [67/100], Loss: 0.7484\n",
            "Epoch [68/100], Loss: 0.7658\n",
            "Epoch [69/100], Loss: 0.7266\n",
            "Epoch [70/100], Loss: 0.7179\n",
            "Epoch [71/100], Loss: 0.7398\n",
            "Epoch [72/100], Loss: 0.7435\n",
            "Epoch [73/100], Loss: 0.7357\n",
            "Epoch [74/100], Loss: 0.7024\n",
            "Epoch [75/100], Loss: 0.7204\n",
            "Epoch [76/100], Loss: 0.7013\n",
            "Epoch [77/100], Loss: 0.7240\n",
            "Epoch [78/100], Loss: 0.6987\n",
            "Epoch [79/100], Loss: 0.7112\n",
            "Epoch [80/100], Loss: 0.7134\n",
            "Epoch [81/100], Loss: 0.7180\n",
            "Epoch [82/100], Loss: 0.7184\n",
            "Epoch [83/100], Loss: 0.7258\n",
            "Epoch [84/100], Loss: 0.6915\n",
            "Epoch [85/100], Loss: 0.6787\n",
            "Epoch [86/100], Loss: 0.7035\n",
            "Epoch [87/100], Loss: 0.6942\n",
            "Epoch [88/100], Loss: 0.6986\n",
            "Epoch [89/100], Loss: 0.6965\n",
            "Epoch [90/100], Loss: 0.6960\n",
            "Epoch [91/100], Loss: 0.6836\n",
            "Epoch [92/100], Loss: 0.6655\n",
            "Epoch [93/100], Loss: 0.6746\n",
            "Epoch [94/100], Loss: 0.6664\n",
            "Epoch [95/100], Loss: 0.6819\n",
            "Epoch [96/100], Loss: 0.6755\n",
            "Epoch [97/100], Loss: 0.6749\n",
            "Epoch [98/100], Loss: 0.6815\n",
            "Epoch [99/100], Loss: 0.6621\n",
            "Epoch [100/100], Loss: 0.6521\n",
            "Mean Squared Error (MSE): 5.8447\n",
            "Root Mean Squared Error (RMSE): 2.4176\n",
            "Mean Absolute Error (MAE): 2.1696\n",
            "R² Score: -2.9559\n",
            "Evaluating iA3...\n",
            "Epoch [1/100], Loss: 2.5825\n",
            "Epoch [2/100], Loss: 1.4790\n",
            "Epoch [3/100], Loss: 1.3596\n",
            "Epoch [4/100], Loss: 1.2462\n",
            "Epoch [5/100], Loss: 1.1370\n",
            "Epoch [6/100], Loss: 1.0587\n",
            "Epoch [7/100], Loss: 1.0121\n",
            "Epoch [8/100], Loss: 0.9418\n",
            "Epoch [9/100], Loss: 0.8981\n",
            "Epoch [10/100], Loss: 0.8606\n",
            "Epoch [11/100], Loss: 0.8192\n",
            "Epoch [12/100], Loss: 0.7869\n",
            "Epoch [13/100], Loss: 0.7687\n",
            "Epoch [14/100], Loss: 0.7184\n",
            "Epoch [15/100], Loss: 0.7106\n",
            "Epoch [16/100], Loss: 0.6953\n",
            "Epoch [17/100], Loss: 0.6481\n",
            "Epoch [18/100], Loss: 0.6458\n",
            "Epoch [19/100], Loss: 0.6365\n",
            "Epoch [20/100], Loss: 0.5914\n",
            "Epoch [21/100], Loss: 0.5996\n",
            "Epoch [22/100], Loss: 0.6077\n",
            "Epoch [23/100], Loss: 0.5922\n",
            "Epoch [24/100], Loss: 0.5743\n",
            "Epoch [25/100], Loss: 0.5442\n",
            "Epoch [26/100], Loss: 0.5588\n",
            "Epoch [27/100], Loss: 0.5003\n",
            "Epoch [28/100], Loss: 0.4965\n",
            "Epoch [29/100], Loss: 0.5110\n",
            "Epoch [30/100], Loss: 0.5012\n",
            "Epoch [31/100], Loss: 0.4770\n",
            "Epoch [32/100], Loss: 0.4363\n",
            "Epoch [33/100], Loss: 0.4782\n",
            "Epoch [34/100], Loss: 0.4453\n",
            "Epoch [35/100], Loss: 0.4741\n",
            "Epoch [36/100], Loss: 0.4411\n",
            "Epoch [37/100], Loss: 0.4414\n",
            "Epoch [38/100], Loss: 0.4246\n",
            "Epoch [39/100], Loss: 0.4323\n",
            "Epoch [40/100], Loss: 0.4275\n",
            "Epoch [41/100], Loss: 0.4281\n",
            "Epoch [42/100], Loss: 0.3925\n",
            "Epoch [43/100], Loss: 0.4212\n",
            "Epoch [44/100], Loss: 0.4364\n",
            "Epoch [45/100], Loss: 0.4042\n",
            "Epoch [46/100], Loss: 0.3913\n",
            "Epoch [47/100], Loss: 0.4018\n",
            "Epoch [48/100], Loss: 0.3840\n",
            "Epoch [49/100], Loss: 0.3799\n",
            "Epoch [50/100], Loss: 0.3637\n",
            "Epoch [51/100], Loss: 0.3522\n",
            "Epoch [52/100], Loss: 0.3953\n",
            "Epoch [53/100], Loss: 0.3854\n",
            "Epoch [54/100], Loss: 0.3779\n",
            "Epoch [55/100], Loss: 0.3586\n",
            "Epoch [56/100], Loss: 0.3571\n",
            "Epoch [57/100], Loss: 0.3442\n",
            "Epoch [58/100], Loss: 0.3389\n",
            "Epoch [59/100], Loss: 0.3483\n",
            "Epoch [60/100], Loss: 0.3336\n",
            "Epoch [61/100], Loss: 0.3352\n",
            "Epoch [62/100], Loss: 0.3428\n",
            "Epoch [63/100], Loss: 0.3560\n",
            "Epoch [64/100], Loss: 0.3450\n",
            "Epoch [65/100], Loss: 0.3262\n",
            "Epoch [66/100], Loss: 0.3384\n",
            "Epoch [67/100], Loss: 0.3234\n",
            "Epoch [68/100], Loss: 0.3251\n",
            "Epoch [69/100], Loss: 0.3261\n",
            "Epoch [70/100], Loss: 0.3454\n",
            "Epoch [71/100], Loss: 0.3345\n",
            "Epoch [72/100], Loss: 0.3105\n",
            "Epoch [73/100], Loss: 0.3110\n",
            "Epoch [74/100], Loss: 0.3027\n",
            "Epoch [75/100], Loss: 0.2904\n",
            "Epoch [76/100], Loss: 0.2936\n",
            "Epoch [77/100], Loss: 0.2984\n",
            "Epoch [78/100], Loss: 0.3114\n",
            "Epoch [79/100], Loss: 0.3118\n",
            "Epoch [80/100], Loss: 0.2930\n",
            "Epoch [81/100], Loss: 0.2748\n",
            "Epoch [82/100], Loss: 0.2784\n",
            "Epoch [83/100], Loss: 0.2866\n",
            "Epoch [84/100], Loss: 0.2836\n",
            "Epoch [85/100], Loss: 0.2753\n",
            "Epoch [86/100], Loss: 0.2681\n",
            "Epoch [87/100], Loss: 0.2732\n",
            "Epoch [88/100], Loss: 0.2663\n",
            "Epoch [89/100], Loss: 0.2524\n",
            "Epoch [90/100], Loss: 0.2653\n",
            "Epoch [91/100], Loss: 0.2635\n",
            "Epoch [92/100], Loss: 0.2551\n",
            "Epoch [93/100], Loss: 0.2735\n",
            "Epoch [94/100], Loss: 0.2473\n",
            "Epoch [95/100], Loss: 0.2576\n",
            "Epoch [96/100], Loss: 0.2467\n",
            "Epoch [97/100], Loss: 0.2541\n",
            "Epoch [98/100], Loss: 0.2433\n",
            "Epoch [99/100], Loss: 0.2579\n",
            "Epoch [100/100], Loss: 0.2569\n",
            "Mean Squared Error (MSE): 2.7468\n",
            "Root Mean Squared Error (RMSE): 1.6573\n",
            "Mean Absolute Error (MAE): 1.4321\n",
            "R² Score: -0.8591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Best Fine-Tuning Strategy\n",
        "\n",
        "\n",
        "{BitFit} (Bias-Term Fine-Tuning) – This method yielded the best performance, achieving a significant improvement in MSE, RMSE, and $R^2$ scores, demonstrating that fine-tuning only the bias terms is an efficient and effective approach."
      ],
      "metadata": {
        "id": "d1xO_Aph6XYf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f42j_ZjKYFSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}