Ramin Yazdani | molformer-finetuning-property-prediction | main | feat(training): Fix OOM: Reduce batch size from 32 to 16

Reduced training batch size from 32 to 16 to fit within GPU memory constraints, resolving
the CUDA out of memory error. Added explanatory comment about GPU memory limits.

Changes:
- batch_size: 32 â†’ 16
- Added comment: "Reduced from 32 due to GPU memory constraints"
- Adjusted gradient accumulation if needed

This is a standard fix in transformer training. Batch size 16 is reasonable for fine-tuning
large language models on consumer/research GPUs. The reduced batch size may increase training
time slightly but ensures successful execution.

Verification: Batch size updated, training can proceed without OOM errors.
